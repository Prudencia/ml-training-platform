version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8081:8000"
    volumes:
      - ./backend/storage:/app/storage
      - ./backend/database:/app/database
      - /var/run/docker.sock:/var/run/docker.sock  # For DetectX ACAP builds
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama
    shm_size: '8gb'  # Increase shared memory for PyTorch DataLoader workers
    restart: unless-stopped
    # GPU support enabled by default (requires nvidia-container-toolkit)
    # Comment out the deploy section below if you don't have an NVIDIA GPU
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      # No VITE_API_URL needed - nginx proxies /api/* to backend container
      # This allows the platform to work from any hostname
    ports:
      - "3080:80"
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  storage:
  database:
  ollama_data:
