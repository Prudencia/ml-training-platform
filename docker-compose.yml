version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    network_mode: host  # Use host networking so backend can reach Ollama on localhost
    volumes:
      - ./backend/storage:/app/storage
      - ./backend/database:/app/database
    environment:
      - PYTHONUNBUFFERED=1
    shm_size: '8gb'  # Increase shared memory for PyTorch DataLoader workers
    restart: unless-stopped
    # GPU support enabled by default (requires nvidia-container-toolkit)
    # Comment out the deploy section below if you don't have an NVIDIA GPU
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      # No VITE_API_URL needed - nginx proxies /api/* to backend container
      # This allows the platform to work from any hostname
    ports:
      - "3080:80"
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  storage:
  database:
